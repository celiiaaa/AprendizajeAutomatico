{"cells":[{"cell_type":"markdown","metadata":{"id":"dmFfJcCKok9J"},"source":["# Q-Learning con FrozenLake-v1 ‚õÑ and Taxi-v3 üöï\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Environments\"/>\n","\n","### üéÆ Entornos:\n","\n","- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n","- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n","\n","### üìö RL-Library:\n","\n","- Python and NumPy\n","- [Gymnasium](https://gymnasium.farama.org/)"]},{"cell_type":"markdown","metadata":{"id":"qa04YZPwpE16"},"source":["## Un peque√±o resumen de Q-Learning"]},{"cell_type":"markdown","metadata":{"id":"Pon7NOGIpXGB"},"source":["*Q-Learning* **es un algoritmo de aprendizaje por refuerzo que**:\n","\n","- Aprende la funci√≥n-Q, una funci√≥n acci√≥n-valor que codifica una tabla Q que contiene todos los valores de los pares estado-acci√≥n de un problema.\n","\n","\n","- Dado un estado y una acci√≥n, nuestra funci√≥n Q buscar√° en la tabla Q el valor correspondiente\n","    \n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\n","\n","- Una vez finalizado el entrenamiento, tendremos una funci√≥n Q √≥ptima y, por tanto, una tabla Q √≥ptima.\n","    \n","- Y si tenemos una funci√≥n Q √≥ptima, tenemos una pol√≠tica √≥ptima, ya que sabemos para, cada estado, la mejor acci√≥n a tomar.\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"  width=\"100%\"/>\n","\n","\n","Pero, al principio, la Q-Table no es √∫til ya que da un valor arbitrario para cada par estado-acci√≥n (la mayor√≠a de las veces inicializamos la Q-Table con valores 0). Pero, a medida que exploremos el entorno y actualicemos nuestra Q-Table nos dar√° mejores aproximaciones\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n","\n","Aqu√≠ tienes el pseudoc√≥digo para Q-Learning:\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>"]},{"cell_type":"markdown","metadata":{"id":"y71fTl2wrBRv"},"source":["## Instalar dependencias y crear un display virtual üîΩ\n","\n","En el notebook, vamos a necesitar generar un v√≠deo para su visualizaci√≥n. Para ello, con Colab, **necesitamos disponer de una pantalla virtual para renderizar el entorno** (y as√≠ grabar la secuencia de las simulaciones).\n","\n","Tenemos que instalar:\n","\n","- `gymnasium`: Contiene los entornos FrozenLake-v1 ‚õÑ y Taxi-v3 üöï.\n","- `pygame`: Necesarios para el interfaz gr√°fico de FrozenLake-v1 y Taxi-v3.\n","- `numpy`: Necesario para gestionar la Q-table.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6GdS1K34n64Z"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 953 kB 3.6 MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /Users/celia/miniconda3/lib/python3.8/site-packages (from gymnasium) (1.24.4)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/celia/miniconda3/lib/python3.8/site-packages (from gymnasium) (7.0.1)\n","Collecting cloudpickle>=1.2.0\n","  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /Users/celia/miniconda3/lib/python3.8/site-packages (from gymnasium) (4.9.0)\n","Collecting farama-notifications>=0.0.1\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: zipp>=0.5 in /Users/celia/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.17.0)\n","Installing collected packages: farama-notifications, cloudpickle, gymnasium\n","Successfully installed cloudpickle-3.0.0 farama-notifications-0.0.4 gymnasium-0.29.1\n","Collecting pygame\n","  Downloading pygame-2.5.2-cp38-cp38-macosx_11_0_arm64.whl (12.3 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.3 MB 4.8 MB/s eta 0:00:01\n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-2.5.2\n","Collecting mediapy\n","  Downloading mediapy-1.2.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: Pillow in /Users/celia/miniconda3/lib/python3.8/site-packages (from mediapy) (10.2.0)\n","Requirement already satisfied: ipython in /Users/celia/miniconda3/lib/python3.8/site-packages (from mediapy) (8.12.0)\n","Requirement already satisfied: matplotlib in /Users/celia/miniconda3/lib/python3.8/site-packages (from mediapy) (3.7.5)\n","Requirement already satisfied: numpy in /Users/celia/miniconda3/lib/python3.8/site-packages (from mediapy) (1.24.4)\n","Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (3.0.42)\n","Requirement already satisfied: jedi>=0.16 in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (0.19.1)\n","Requirement already satisfied: pickleshare in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (0.7.5)\n","Requirement already satisfied: backcall in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (0.2.0)\n","Requirement already satisfied: pygments>=2.4.0 in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (2.17.2)\n","Requirement already satisfied: matplotlib-inline in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (0.1.6)\n","Requirement already satisfied: appnope in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (0.1.4)\n","Requirement already satisfied: decorator in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (5.1.1)\n","Requirement already satisfied: pexpect>4.3 in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (4.9.0)\n","Requirement already satisfied: stack-data in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (0.6.2)\n","Requirement already satisfied: typing-extensions in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (4.9.0)\n","Requirement already satisfied: traitlets>=5 in /Users/celia/miniconda3/lib/python3.8/site-packages (from ipython->mediapy) (5.14.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/celia/miniconda3/lib/python3.8/site-packages (from jedi>=0.16->ipython->mediapy) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /Users/celia/miniconda3/lib/python3.8/site-packages (from pexpect>4.3->ipython->mediapy) (0.7.0)\n","Requirement already satisfied: wcwidth in /Users/celia/miniconda3/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->mediapy) (0.2.13)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /Users/celia/miniconda3/lib/python3.8/site-packages (from matplotlib->mediapy) (1.4.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /Users/celia/miniconda3/lib/python3.8/site-packages (from matplotlib->mediapy) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /Users/celia/miniconda3/lib/python3.8/site-packages (from matplotlib->mediapy) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /Users/celia/miniconda3/lib/python3.8/site-packages (from matplotlib->mediapy) (4.49.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /Users/celia/miniconda3/lib/python3.8/site-packages (from matplotlib->mediapy) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /Users/celia/miniconda3/lib/python3.8/site-packages (from matplotlib->mediapy) (2.8.2)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /Users/celia/miniconda3/lib/python3.8/site-packages (from matplotlib->mediapy) (6.1.1)\n","Requirement already satisfied: packaging>=20.0 in /Users/celia/miniconda3/lib/python3.8/site-packages (from matplotlib->mediapy) (23.2)\n","Requirement already satisfied: zipp>=3.1.0 in /Users/celia/miniconda3/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->mediapy) (3.17.0)\n","Requirement already satisfied: six>=1.5 in /Users/celia/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->mediapy) (1.16.0)\n","Requirement already satisfied: executing>=1.2.0 in /Users/celia/miniconda3/lib/python3.8/site-packages (from stack-data->ipython->mediapy) (2.0.1)\n","Requirement already satisfied: asttokens>=2.1.0 in /Users/celia/miniconda3/lib/python3.8/site-packages (from stack-data->ipython->mediapy) (2.4.1)\n","Requirement already satisfied: pure-eval in /Users/celia/miniconda3/lib/python3.8/site-packages (from stack-data->ipython->mediapy) (0.2.2)\n","Installing collected packages: mediapy\n","Successfully installed mediapy-1.2.0\n"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install mediapy # Para reproducir v√≠deos"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"JpTAGHRgxw-n"},"outputs":[{"name":"stdout","output_type":"stream","text":["Password:\n","sudo: a password is required\n","Password:"]}],"source":["!sudo apt-get update\n","!sudo apt-get install -y python3-opengl\n","!apt install ffmpeg xvfb\n","!pip3 install pyvirtualdisplay"]},{"cell_type":"markdown","metadata":{"id":"a5QQwcXzyIOW"},"source":["Para asegurarse de que se utilizan las nuevas bibliotecas instaladas, a veces es necesario reiniciar el runtime del entorno. La siguiente celda forzar√° al runtime a bloquearse, por lo que tendr√° que reconectarse de nuevo. Gracias a este truco, podremos ejecutar el display virtual."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QX-LRK9NyVtE"},"outputs":[],"source":["import os\n","os.kill(os.getpid(), 9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aE9ixk3BydYE"},"outputs":[],"source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"]},{"cell_type":"markdown","metadata":{"id":"2-wSZbdFyjrv"},"source":["## Importamos los paquetes üì¶\n","\n","In addition to the installed libraries, we also use:\n","\n","- `random`: generar n√∫meros aleatorios (ser√° √∫tilo para la pol√≠tica epsilon-greedy).\n","- `imageio`: Manejar el v√≠deo que hagamos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOXTFuoezC-v"},"outputs":[],"source":["import numpy as np\n","import gymnasium as gym\n","import random\n","import imageio\n","import os\n","import tqdm\n","\n","from tqdm.notebook import tqdm"]},{"cell_type":"markdown","metadata":{"id":"lWER47z8zSJl"},"source":["# Parte 1: Frozen Lake ‚õÑ (versi√≥n no resbaladiza)"]},{"cell_type":"markdown","metadata":{"id":"1KqrQ0FYz0mQ"},"source":["## Entendiendo [Entorno FrozenLake ‚õÑ]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n","---\n","\n","üí° Una buena costumbre cuando empiezas a usar un entorno es consultar su documentaci√≥n\n","\n","üëâ https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n","\n","---\n","\n","Vamos a entrenar a nuestro agente Q-Learning **para que navegue desde el estado inicial (S) hasta el estado meta (G) caminando s√≥lo sobre baldosas congeladas (F) y evitando los agujeros (H)**.\n","\n","Podemos tener dos tama√±os de entorno:\n","\n","- `map_name=\"4x4\"`: una versi√≥n de cuadr√≠cula 4x4\n","- `map_name=\"8x8\"`: una versi√≥n en cuadr√≠cula de 8x8\n","\n","\n","El entorno tiene dos modos:\n","\n","- `is_slippery=False`: El agente siempre se mueve **en la direcci√≥n prevista** debido a la naturaleza no resbaladiza del lago helado (determinista).\n","- `is_slippery=True`: El agente **puede no moverse siempre en la direcci√≥n prevista** debido a la naturaleza resbaladiza del lago helado (estoc√°stico)."]},{"cell_type":"markdown","metadata":{"id":"ufvri27f0YcS"},"source":["### Soluci√≥n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DN-qyT4q0e7-"},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{"id":"T3VqFWdz0pKg"},"source":["Podemos crear nuestro propio grid como se muestra a continuaci√≥n:\n","\n","```python\n","desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n","gym.make('FrozenLake-v1', desc=desc, is_slippery=False)\n","```\n","\n","por ahora, cogemos el entorno por defecto.\n"]},{"cell_type":"markdown","metadata":{"id":"QiJHdQaf0-Jt"},"source":["### Veamos c√≥mo es el entorno:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkRvDwlu1Fwm"},"outputs":[],"source":["# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n","print(\"_____OBSERVATION SPACE_____ \\n\")\n","print(\"Observation Space\", env.observation_space)\n","print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"]},{"cell_type":"markdown","metadata":{"id":"zTdf0rUQ1i3T"},"source":["`Observation Space Shape Discrete(16)` indica que la observaci√≥n es un entero que representa la **posici√≥n actual del agente como fila_actual * ncols + col_actual (donde tanto la fila como la col empiezan en 0)**.\n","\n","Por ejemplo, la posici√≥n de la meta en el mapa 4x4 se puede calcular de la siguiente manera: 3 * 4 + 3 = 15. El n√∫mero de observaciones posibles depende del tama√±o del mapa. **Por ejemplo, el mapa 4x4 tiene 16 observaciones posibles**.\n","\n","Por ejemplo, esto es lo que representar√≠a con `state = 0`:\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kxI48SN2L3E"},"outputs":[],"source":["print(\"\\n _____ACTION SPACE_____ \\n\")\n","print(\"Action Space Shape\", env.action_space.n)\n","print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"]},{"cell_type":"markdown","metadata":{"id":"sfZ0vSpv2fqR"},"source":["El espacio de acciones (el conjunto de acciones posibles que puede realizar el agente) es discreto con 4 acciones disponibles üéÆ:\n","- 0: IR A LA IZQUIERDA\n","- 1: IR ABAJO\n","- 2: IR A LA DERECHA\n","- 3: IR ARRIBA\n","\n","Funci√≥n de recompensa üí∞:\n","- Llegar a la meta: +1\n","- Caer en un agujero: 0\n","- Quedarte congelado: 0"]},{"cell_type":"markdown","metadata":{"id":"BxuaaQGz26j9"},"source":["## Crear e inicializar la tabla Q üóÑÔ∏è\n","\n","(üëÄ Paso 1 del pseudoc√≥digo)\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Aprendizaje\" width=\"100%\"/>\n","\n","\n","Para saber cu√°ntas filas (estados) y columnas (acciones) a utilizar, necesitamos saber las acciones y el n√∫mero de estados del problema. Ya conocemos sus valores de antes, pero vamos a querer obtenerlos para que el algoritmo se generalice para diferentes entornos. Gym nos proporciona una forma de hacerlo: `env.espacio_acci√≥n.n` y `env.espacio_observacion.n`.\n"]},{"cell_type":"markdown","metadata":{"id":"6t-D4Asb3WuJ"},"source":["### Soluci√≥n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"we_JTDCK2eZc"},"outputs":[],"source":["state_space = env.observation_space.n\n","print(\"There are \", state_space, \" possible states\")\n","\n","action_space = env.action_space.n\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpeJ8dnJ3jOD"},"outputs":[],"source":["# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n","def initialize_q_table(state_space, action_space):\n","  Qtable = np.zeros((state_space, action_space))\n","  return Qtable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8w6e18z3p_8"},"outputs":[],"source":["Qtable_frozenlake = initialize_q_table(state_space, action_space)"]},{"cell_type":"markdown","metadata":{"id":"ySAhxGpu3w-7"},"source":["## Definir la mejor pol√≠tica ü§ñ\n","\n","Recuerda que tenemos dos pol√≠ticas ya que Q-Learning es un algoritmo **off-policy**. Esto significa que estamos usando una **pol√≠tica diferente para actuar y actualizar la funci√≥n de valor**.\n","\n","- Pol√≠tica epsilon-greedy (exploraci√≥n)\n","- Pol√≠tica greedy (explotaci√≥n)\n","\n","La pol√≠tica greedy tambi√©n ser√° la pol√≠tica final que tendremos cuando el agente Q-learning complete el entrenamiento. La pol√≠tica greedy se utiliza para seleccionar una acci√≥n utilizando la tabla Q.\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"DfrwYX6V40XT"},"source":["#### Soluci√≥n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UEuCyvQ9YgY"},"outputs":[],"source":["def greedy_policy(Qtable, state):\n","  # Exploitation: take the action with the highest state, action value\n","  action = np.argmax(Qtable[state][:])\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"lvP4Plr79h9A"},"source":["## Definici√≥n de la pol√≠tica epsilon-greedy ü§ñ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_ftCUbn4-t1"},"outputs":[],"source":["def epsilon_greedy_policy(Qtable, state, epsilon):\n","  # Randomly generate a number between 0 and 1\n","  random_num = random.uniform(0,1)\n","  # if random_num > greater than epsilon --> exploitation\n","  if random_num > epsilon:\n","    # Take the action with the highest value given a state\n","    # np.argmax can be useful here\n","    action = greedy_policy(Qtable, state)\n","  # else --> exploration\n","  else:\n","    action = env.action_space.sample()\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"4Kwh7T_t5K2e"},"source":["## Definici√≥n de hiperpar√°metros ‚öôÔ∏è\n","\n","Los hiperpar√°metros relacionados con la exploraci√≥n son algunos de los m√°s importantes.\n","\n","- Tenemos que asegurarnos de que nuestro agente **explora lo suficiente del espacio de estados** para aprender una buena aproximaci√≥n de valores. Para ello, necesitamos tener un decaimiento progresivo del √©psilon.\n","- Si disminuimos epsilon demasiado r√°pido (tasa de decaimiento demasiado alta), **corremos el riesgo de que nuestro agente se quede atascado**, ya que no ha explorado lo suficiente el espacio de estados y, por tanto, no puede resolver el problema.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rvu4NzGI5ziu"},"outputs":[],"source":["# Training parameters\n","n_training_episodes = 10000  # Total training episodes\n","learning_rate = 0.7          # Learning rate\n","\n","# Evaluation parameters\n","n_eval_episodes = 100        # Total number of test episodes\n","\n","# Environment parameters\n","env_id = \"FrozenLake-v1\"     # Name of the environment\n","max_steps = 99               # Max steps per episode\n","gamma = 0.95                 # Discounting rate\n","eval_seed = []               # The evaluation seed of the environment\n","\n","# Exploration parameters\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.05            # Minimum exploration probability\n","decay_rate = 0.0005            # Exponential decay rate for exploration prob"]},{"cell_type":"markdown","metadata":{"id":"jIqDCeIw6FYi"},"source":["## El bucle de entrenamiento\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n","\n","El bucle de entrenamiento ser√≠a como lo siguiente:\n","\n","```\n","Por cada episodio:\n","\n","Reducimos epsilon (ya que cada vez se necesita menos exploraci√≥n)\n","Restablecer el entorno\n","\n","  Para cada paso dentro del n√∫mero m√°ximo (`max_steps`):\n","    Eleginos la acci√≥n At utilizando la pol√≠tica epsilon-greedy\n","    Realiza la acci√≥n (a) y observa el estado resultante (s') y la recompensa (r)\n","    Actualizar el Q-valor Q(s,a) utilizando la ecuaci√≥n de Bellman Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","    Si hemos llegado a un estado final, terminar el episodio\n","    El siguiente estado es el nuevo estado s'\n","```"]},{"cell_type":"markdown","metadata":{"id":"vNGNwqYO7-01"},"source":["#### Soluci√≥n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgbFI2g18OKS"},"outputs":[],"source":["def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n","  for episode in tqdm(range(n_training_episodes)):\n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    # Reset the environment\n","    state, info = env.reset()\n","    step = 0\n","    terminated = False\n","    truncated = False\n","\n","    # repeat\n","    for step in range(max_steps):\n","      # Choose the action At using epsilon greedy policy\n","      action = epsilon_greedy_policy(Qtable, state, epsilon)\n","\n","      # Take action At and observe Rt+1 and St+1\n","      # Take the action (a) and observe the outcome state(s') and reward (r)\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","\n","      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n","\n","      # If terminated or truncated finish the episode\n","      if terminated or truncated:\n","        break\n","\n","      # Our next state is the new state\n","      state = new_state\n","  return Qtable"]},{"cell_type":"markdown","metadata":{"id":"GPVPkrKY8YG0"},"source":["## Entrenando el agente Q-Learning üèÉ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ky5bTfKL8fgm"},"outputs":[],"source":["Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"]},{"cell_type":"markdown","metadata":{"id":"p3zoo9Rn94iQ"},"source":["## Veamos como queda la Q-tabla üëÄ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e26pU4UU-C1t"},"outputs":[],"source":["Qtable_frozenlake"]},{"cell_type":"markdown","metadata":{"id":"XFFel7wJ-iAD"},"source":["## El m√©todo de evaluaci√≥n üìù\n","\n","- Definimos el m√©todo de evaluaci√≥n que vamos a utilizar para probar nuestro agente Q-Learning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cP-cnFja-uU_"},"outputs":[],"source":["def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n","  \"\"\"\n","  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n","  :param env: The evaluation environment\n","  :param max_steps: Maximum number of steps per episode\n","  :param n_eval_episodes: Number of episode to evaluate the agent\n","  :param Q: The Q-table\n","  :param seed: The evaluation seed array (for taxi-v3)\n","  \"\"\"\n","  episode_rewards = []\n","  for episode in tqdm(range(n_eval_episodes)):\n","    if seed:\n","      state, info = env.reset(seed=seed[episode])\n","    else:\n","      state, info = env.reset()\n","    step = 0\n","    truncated = False\n","    terminated = False\n","    total_rewards_ep = 0\n","\n","    for step in range(max_steps):\n","      # Take the action (index) that have the maximum expected future reward given that state\n","      action = greedy_policy(Q, state)\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","      total_rewards_ep += reward\n","\n","      if terminated or truncated:\n","        break\n","      state = new_state\n","    episode_rewards.append(total_rewards_ep)\n","  mean_reward = np.mean(episode_rewards)\n","  std_reward = np.std(episode_rewards)\n","\n","  return mean_reward, std_reward"]},{"cell_type":"markdown","metadata":{"id":"YWZQTAEh--VO"},"source":["## Evaluar nuestro agente Q-Learning üìà\n","\n","- Normalmente, deber√≠a tener una recompensa media de 1.0\n","- El **entorno es relativamente f√°cil** ya que el espacio de estados es realmente peque√±o (16). Lo que puedes intentar es [sustituirlo por la versi√≥n resbaladiza](https://gymnasium.farama.org/environments/toy_text/frozen_lake/), que introduce estocasticidad, haciendo el entorno m√°s complejo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rffmPRI7_Klf"},"outputs":[],"source":["# Evaluate our Agent\n","mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n","print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"QuHp3BYU_RsP"},"source":["#### No modificar este c√≥digo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qdi5-D33_YGy"},"outputs":[],"source":["def record_video(env, Qtable, out_directory, fps=1, max_iter=100):\n","  \"\"\"\n","  Generate a replay video of the agent\n","  :param env\n","  :param Qtable: Qtable of our agent\n","  :param out_directory\n","  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n","  \"\"\"\n","  images = []\n","  terminated = False\n","  truncated = False\n","  state, info = env.reset(seed=random.randint(0,500))\n","  img = env.render()\n","  images.append(img)\n","  iters = 0\n","  while iters<max_iter and (not terminated or truncated):\n","    # Take the action (index) that have the maximum expected future reward given that state\n","    action = np.argmax(Qtable[state][:])\n","    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n","    img = env.render()\n","    images.append(img)\n","    iters = iters+1\n","  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"]},{"cell_type":"markdown","metadata":{"id":"zA-K8oN6_kOc"},"source":["- Vamos a crear **el diccionario del modelo que contiene los hiperpar√°metros y la tabla Q**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YR5oyPS_r7e"},"outputs":[],"source":["model = {\n","    \"env_id\": env_id,\n","    \"max_steps\": max_steps,\n","    \"n_training_episodes\": n_training_episodes,\n","    \"n_eval_episodes\": n_eval_episodes,\n","    \"eval_seed\": eval_seed,\n","\n","    \"learning_rate\": learning_rate,\n","    \"gamma\": gamma,\n","\n","    \"max_epsilon\": max_epsilon,\n","    \"min_epsilon\": min_epsilon,\n","    \"decay_rate\": decay_rate,\n","\n","    \"qtable\": Qtable_frozenlake\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5lDqT51_8lP"},"outputs":[],"source":["evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n","\n","readme_path = \".\"\n","# Step 6: Record a video\n","video_path = \"replay.mp4\"\n","record_video(env, model[\"qtable\"], video_path, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q66nEsMEAJ11"},"outputs":[],"source":["import mediapy as media\n","video = media.read_video('replay.mp4')\n","media.show_video(video)"]},{"cell_type":"markdown","metadata":{"id":"_PSODRL_NG1T"},"source":["# Parte 2: Taxi-v3 üöñ\n","\n","## Entendiendo [Taxi-v3 üöï](https://gymnasium.farama.org/environments/toy_text/taxi/)\n","---\n","\n","üí° Una buena costumbre cuando se empieza a utilizar un entorno es consultar su documentaci√≥n\n","\n","üëâ https://gymnasium.farama.org/environments/toy_text/taxi/\n","\n","---\n","En `Taxi-v3` üöï, hay cuatro lugares designados en el mundo cuadriculado indicados por R(ed), G(reen), Y(ellow) y B(lue).\n","\n","Cuando comienza el episodio, **el taxi parte de una casilla aleatoria** y el pasajero se encuentra en un lugar aleatorio. El taxi se dirige al lugar donde se encuentra el pasajero, **recoge al pasajero**, se dirige al destino del pasajero (otro de los cuatro lugares especificados) y **deja al pasajero**. Una vez que se deja al pasajero, el episodio termina.\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oi_fhYHzNpK8"},"outputs":[],"source":["env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{"id":"Q_o3CMzMN2ld"},"source":["Hay **500 estados discretos, ya que hay 25 posiciones del taxi, 5 posibles ubicaciones del pasajero** (incluido el caso en que el pasajero est√° en el taxi) y **4 ubicaciones de destino.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ByWrm0qGONcx"},"outputs":[],"source":["state_space = env.observation_space.n\n","print(\"There are \", state_space, \" possible states\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxcts4AvOVh9"},"outputs":[],"source":["action_space = env.action_space.n\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"markdown","metadata":{"id":"9a9cCKhgOpvD"},"source":["El espacio de acciones (el conjunto de acciones posibles que puede realizar el agente) es discreto con **6 acciones disponibles üéÆ**:\n","\n","- 0: desplazarse al sur\n","- 1: hacia el norte\n","- 2: desplazarse al este\n","- 3: desplazarse al oeste\n","- 4: recoger pasajero\n","- 5: dejar pasajero\n","\n","Funci√≥n de recompensa üí∞:\n","\n","- -1 por paso, a menos que se active otra recompensa.\n","- +20 por entregar pasajero.\n","- -10 ejecutando acciones \"recoger\" y \"dejar\" ilegalmente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0Ifpk2fPP_U"},"outputs":[],"source":["# Create our Q table with state_size rows and action_size columns (500x6)\n","Qtable_taxi = initialize_q_table(state_space, action_space)\n","print(Qtable_taxi)\n","print(\"Q-table shape: \", Qtable_taxi .shape)"]},{"cell_type":"markdown","metadata":{"id":"n6gV9OrtPzjo"},"source":["## Definir los hiperpar√°metros ‚öôÔ∏è\n","\n","‚ö† NO MODIFICAR EVAL_SEED: el array eval_seed **nos permite evaluar un agente con las mismas posiciones de partida de taxi para cada ejecuci√≥n**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMXoR1uuP_BJ"},"outputs":[],"source":["# Training parameters\n","n_training_episodes = 25000   # Total training episodes\n","learning_rate = 0.7           # Learning rate\n","\n","# Evaluation parameters\n","n_eval_episodes = 100        # Total number of test episodes\n","\n","# DO NOT MODIFY EVAL_SEED\n","eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n"," 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n"," 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n","                                                          # Each seed has a specific starting state\n","\n","# Environment parameters\n","env_id = \"Taxi-v3\"           # Name of the environment\n","max_steps = 99               # Max steps per episode\n","gamma = 0.95                 # Discounting rate\n","\n","# Exploration parameters\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.05           # Minimum exploration probability\n","decay_rate = 0.005            # Exponential decay rate for exploration prob"]},{"cell_type":"markdown","metadata":{"id":"LUcFHgXbQHa8"},"source":["## Entrenamos el agente Q-Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQ39V4W_QO3N"},"outputs":[],"source":["Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n","Qtable_taxi"]},{"cell_type":"markdown","metadata":{"id":"dxi346mKQoHn"},"source":["Evaluamos el agente Q-Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48YZQKuCQrVi"},"outputs":[],"source":["# Evaluate our Agent\n","mean_reward_taxi, std_reward_taxi = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_taxi, eval_seed)\n","print(f\"Mean_reward={mean_reward_taxi:.2f} +/- {std_reward_taxi:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQU37KVDRba4"},"outputs":[],"source":["model = {\n","    \"env_id\": env_id,\n","    \"max_steps\": max_steps,\n","    \"n_training_episodes\": n_training_episodes,\n","    \"n_eval_episodes\": n_eval_episodes,\n","    \"eval_seed\": eval_seed,\n","\n","    \"learning_rate\": learning_rate,\n","    \"gamma\": gamma,\n","\n","    \"max_epsilon\": max_epsilon,\n","    \"min_epsilon\": min_epsilon,\n","    \"decay_rate\": decay_rate,\n","\n","    \"qtable\": Qtable_taxi\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_wVoqWeRoCq"},"outputs":[],"source":["#evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n","\n","readme_path = \".\"\n","# Step 6: Record a video\n","video_path = \"replay_taxi.mp4\"\n","record_video(env, model[\"qtable\"], video_path, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VeQrP2-uRtHK"},"outputs":[],"source":["import mediapy as media\n","video = media.read_video('replay_taxi.mp4')\n","media.show_video(video)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}
